###week01_ex1_start_py
print('Exercise 1')
###week01_ex1_end

###week01_ex2_start_md
* `+`, `-` are addition and subtraction,
* `*`, `/` are multiplication and division,
* `**` is exponentiation (e.g. `2 ** 3` returns $2^3$),
* `//` is integer division,
* `%` is modulus, e.g. `38 % 5` returns the remainder of `38 // 5`.
###week01_ex2_end

###week01_ex3_start_py
import numpy as np
print(np.sqrt(2) * np.cos(2 * np.pi / 5))
###week01_ex3_end

###week01_ex5_start_py
my_string = 'Some text characters of my choice.'
m = 6
n = 3

print(my_string[m + n - 1])
print(my_string[m - n - 1])
###week01_ex5_end

###week01_ex6_start_py
u = 3
v = 5

# One possible way...
same_sign = (u > 0 and v > 0) or (u < 0 and v < 0)
print(same_sign)

# Another way...
same_sign = u * v > 0
print(same_sign)
###week01_ex6_end

###week01_ex7_start_py
x = 1.110223024625156e-16

print(x == 0)
print(x + 1 == 1)
###week01_ex7_switch_md
The value given is (just under) half of what we call $\epsilon$, or **machine epsilon** ---
the distance between 1 and the next closest number in double-precision floating-point representation
(i.e. using the `float` type).

This means that any real number $a \in [1, 1+\frac{\epsilon}{2})$ has the same floating-point
representation as $1$ -- it effectively gets rounded down to 1. Similarly, any real number
$b \in [1 + \frac{\epsilon}{2}, 1 + \epsilon]$ has the same representation as $1 + \epsilon$,
because it is rounded up. Now, working in finite precision means that we can actually find
*the* largest floating-point number `x`$>0$ such that `x`$<\frac{\epsilon}{2}$ -- this is the
value given here.

For a simpler example, if we worked with a system which could only represent numbers up to
3 significant digits:
* We could represent the number $1$ and the number $1.01$, but every real number in between
would get rounded to the closest of these values.
* This means that, for this system, $\epsilon=0.01$.
* The largest number `x`$>0$ representable in this system such that `x`$<\frac{\epsilon}{2}=0.005$
is `x = 0.00499`, since we can store at most 3 significant digits.

Note that, in this system, there is no difference between the value of `1 + 0.001`, `1 + 0.00136`,
`1 + 0.00487`, etc. --- they all evaluate to `1`. We say that [floating-point addition leads to
**round-off error**](https://en.wikipedia.org/wiki/Round-off_error#Addition).

Numpy can display the value of $\epsilon$ (and other information about number objects) using `np.finfo()`
([documentation](https://numpy.org/devdocs/reference/generated/numpy.finfo.html#numpy-finfo)),
try these commands:
###week01_ex7_switch_py
print(np.finfo(1.0))
np.finfo(1.0).eps
###week01_ex7_end
